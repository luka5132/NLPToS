{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Interpretting Privbert Hierarchical Classification Result\n## Lukas Busch (AUC)\n### \n\nCreated on Kaggle:\nhttps://www.kaggle.com/lukasbusch/privbert-results\n\nThis notebook was used in combination with the privbert-data dataset that contains classification results and label support data for my capstone (https://github.com/luka5132/NLPToS) on multi-label classification on the OPP-115 dataset (https://www.usableprivacy.org/data)\nScroll down to see how:\n- The 5-fold data was combined to create final results\n- The labels support for the data was calculated\n...\n","metadata":{}},{"cell_type":"code","source":"# loading the needed libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label distribution:\n\nWith the code below the mean label distribution of the 5-fold validation sets were calculated","metadata":{}},{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has the correct suffix (.py mostly))\ncopyfile(src = \"../input/privbert-data/data_processing.py\", dst = \"../working/data_processing.py\")\ncopyfile(src = \"../input/privbert-data/pytorch_classifier.py\", dst = \"../working/pytorch_classifier.py\")\ncopyfile(src = \"../input/privbert-data/hierarchical_data.py\", dst = \"../working/hierarchical_data.py\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from data_processing import Op115OneHots\nALL_PATH = '../input/privbert-data/op115_processed.csv'\n\n#reading all data so that we know how many different classes there are for each level:\n    # Categories     10\n    # Subcategories  36\n    # Values         259\n    \n#process the data using the Op115OneHots class located in data_processing.py\nop115_all = pd.read_csv(ALL_PATH)\nop115_all_c = Op115OneHots(op115_all)\nop115_all_c.go2(majority = True)\n\n#get the unique labels, since some labels are sparse is is possible that a value label might not appear in the test or train\n#set, that is why I initiate every class with all labels so that all one hot vectors are the same length (as they should be)\nuniques = op115_all_c.return_oh_names()\ncatsub_index, catval_index, subval_index, inds = op115_all_c.len_onehots()\n\n#name per category (notice how it is sorted, thus it is not in the same order as in the paper)\ncat_names = sorted(op115_all_c.unique_cats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiate the dataframe where the respective values will be stores\nCOLNAMES = ['k', 'class', 'train_size', 'val_size', 'test_size']\nlabel_df = pd.DataFrame(columns = COLNAMES , dtype=float)\n\nfor i in range(5):\n    #Reading each of the 5 datasets that were used\n    TRAIN_DATAPATH = '../input/privbert-data/op115_data/op115_train_k{}.csv'.format(i)\n    VAL_DATAPATH = '../input/privbert-data/op115_data/op115_val_k{}.csv'.format(i)\n    TEST_DATAPATH = '../input/privbert-data/op115_data/op115_test_k{}.csv'.format(i)\n    traindf = pd.read_csv(TRAIN_DATAPATH)\n    valdf = pd.read_csv(VAL_DATAPATH)\n    testdf = pd.read_csv(TEST_DATAPATH)\n    \n    #get the multi-label one hot vectors (train_cats, val_cats and test_cats)\n    op115_tr = Op115OneHots(traindf)\n    op115_tr.go2(majority = True, class_tup = uniques)\n\n    t_catsub,t_catval,t_subval,train_cats,t_subs,t_vals,t_my_texts = op115_tr.new_onehots()\n    \n    op115_val = Op115OneHots(valdf)\n    op115_val.go2(majority = True, class_tup = uniques)\n\n    t_catsub,t_catval,t_subval,val_cats,t_subs,t_vals,t_my_texts = op115_val.new_onehots()\n    \n    op115_te = Op115OneHots(testdf)\n    op115_te.go2(majority = True, class_tup = uniques)\n\n    t_catsub,t_catval,t_subval,test_cats,t_subs,t_vals,t_my_texts = op115_te.new_onehots()\n    \n    #turn type : list onehot vecotrs to type : np.array\n    traincats = np.array(train_cats)\n    valcats = np.array(val_cats)\n    testcats = np.array(test_cats)\n    print('set {}:'.format(i))\n    for j in range(10):\n        #For each colun count the number of labels by summing that column (label is either 0 or 1)\n        cat_name = cat_names[j] #get the name of the categry\n        train_labels = sum(traincats[:,j]) #num of labels for train set\n        val_labels = sum(valcats[:,j]) # ''  val set\n        test_labels = sum(testcats[:,j]) # '' test set\n        \n        # create a row for our dataframe\n        row = [i,cat_name,train_labels,val_labels,test_labels]\n        label_df.loc[len(label_df)] = row","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_df.to_csv('label_distribution.csv') #save the df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset\nlabeldist = pd.read_csv('../input/privbert-data/label_distribution.csv')\nlabeldist = labeldist.groupby(['class']).mean() # grouping the values per class\nlabeldist['all_size'] = labeldist['train_size'] + labeldist['test_size'] +labeldist['val_size'] # adding column with all values\ntotsum = sum(labeldist['all_size']) # total number of labels\nprint(totsum)\nlabeldist['tot %'] = labeldist['all_size'] / totsum # % of labels for a category with respect to total\nlabeldist.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total labels per set (train / val / test)\nprint(sum(labeldist['all_size']))\nprint(sum(labeldist['train_size']))\nprint(sum(labeldist['val_size']))\nprint(sum(labeldist['test_size']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick check to see how well our data is stratified for each testing set (not really needed after code aoe)\nfor i in range(5):\n    DATAPATH = '../input/privbert-data/op115_data/op115_test_k{}.csv'.format(i)\n    testdf = pd.read_csv(DATAPATH)\n    \n    op115_t_c = Op115OneHots(testdf)\n    op115_t_c.go2(majority = True, class_tup = uniques)\n\n    t_catsub,t_catval,t_subval,t_cats,t_subs,t_vals,t_my_texts = op115_t_c.new_onehots()\n    numpcats = np.array(t_cats)\n    print('testset {}:'.format(i))\n    for j in range(10):\n        print(sum(numpcats[:,j]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The following code calculated the f1 scores for the 5 test sets\n\nIt is not completetly automated, instead I loaded the 5 different prediction files seperately and combined them later. That way I didn't really need to copy my code too many times.","metadata":{}},{"cell_type":"code","source":"# start with k = 0.\n# In general I do: k in range(5) but then manually\nk = 0\nPRED_PATH = '../input/privbert-data/predictions_k{}.csv'.format(k)\npred_df = pd.read_csv(PRED_PATH) #read prediction\npred_df = pred_df.drop(['Unnamed: 0'],axis=1) \npred_df.head()\n\nadvice_names = pred_df.advice_name.unique() #number of different advice layers \npred_cols = pred_df.columns[1:11] # scores for the predicted labels\ntrue_cols = pred_df.columns[11:] # actuall (true) labels\n \nprint(pred_cols,true_cols) # names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dict = {} \nfor adv_name in advice_names:\n    adv_df = pred_df[pred_df['advice_name'] == adv_name] # get the scores for the respective advice method\n    preds = adv_df[pred_cols].values\n    \n    true_labels = adv_df[true_cols].values.tolist()\n    pred_bools = [pl>0.6 for pl in preds] # transform predictions into bools using the final treshold of 0.6 as obtained\n                                          # by gridsearch\n   \n    val_f1_accuracy = f1_score(true_labels,pred_bools,average='macro')*100 #only for printing\n    val_flat_accuracy = accuracy_score(true_labels, pred_bools)*100 #only for printing\n\n    print('F1 Validation Accuracy: ', val_f1_accuracy)\n    print('Flat Validation Accuracy: ', val_flat_accuracy)\n    \n    # save the scores using the 'classification_report' function from\n    clf_report = classification_report(true_labels,pred_bools, target_names = pred_cols, output_dict =True) \n    result_dict[adv_name] = clf_report\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#COLUMNS = ['class', 'support'] + list(advice_names)\ndef get_nice_res(k):\n    \"\"\"\n    Function that uses the classification result dictionary and turns it into a nice easy to use dataframe\n    \"\"\"\n    new_df = pd.DataFrame()\n    class_and_sup = False\n    for adv_name in advice_names:\n        subdict = result_dict[adv_name]\n        classname_list = []\n        f1_scores = []\n        support_list = []\n        for classname in sorted(subdict.keys()):\n            class_dict = subdict[classname]\n            f1val = class_dict['f1-score']\n            support = class_dict['support']\n\n            if not class_and_sup:\n                classname_list.append(classname)\n                support_list.append(support)\n\n            f1_scores.append(f1val)\n\n        if not class_and_sup:\n            new_df['class'] = classname_list\n            new_df['support'] = support_list\n            new_df['k'] = [k] * len(support_list)\n            class_and_sup = True\n        new_df[adv_name] = f1_scores\n    return new_df\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k0 = get_nice_res(k) # save results for k0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k1 = get_nice_res(k) # '' k1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k2 = get_nice_res(k) # '' k2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k3 = get_nice_res(k) # '' k3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k4 = get_nice_res(k) # '' k4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding the df's together\n\n# alldf = k0.append(k1, ignore_index = True)\n# alldf = alldf.append(k2, ignore_index = True)\n# alldf = alldf.append(k3, ignore_index = True)\nalldf = alldf.append(k4, ignore_index = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json \nall_k_dict = {}\nfor i in range(5):\n    pred_df = pd.read_csv('../input/privbert-data/predictions_k{}.csv'.format(i))\n    advice_names = pred_df.advice_name.unique() #number of different advice layers \n    pred_cols = pred_df.columns[2:12] # scores for the predicted labels\n    true_cols = pred_df.columns[12:] # actual (true) labels\n    adv_names = pred_df['advice_name'].unique()\n    k_dict = {}\n    for adv in adv_names:\n        adv_df = pred_df[pred_df['advice_name'] == adv]\n        preds = adv_df[pred_cols].values\n        bools = [p > 0.6 for p in preds]\n        trues = list(adv_df[true_cols].values)\n        clf = classification_report(trues,bools, target_names = pred_cols, output_dict = True)\n        k_dict[adv] = clf\n        \n    all_k_dict[i] = k_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_k_dict[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resdict = {}\nrelevant_names = ['cat', 'sub_val']\nfor i in range(5):\n    res = all_k_dict[i]\n    for adv in relevant_names:\n        adv_dict = res[adv] \n        adv_res = {}\n        for akey in adv_dict.keys():\n            prec = adv_dict[akey]['precision']\n            rec = adv_dict[akey]['recall']\n            if adv in resdict:\n                resdict[adv][akey]['precision'].append(prec)\n                resdict[adv][akey]['recall'].append(rec)\n            if akey in adv_res:\n                adv_res[akey]['precision'].append(prec)\n                adv_res[akey]['recall'].append(rec)\n            else:\n                insdict = {}\n                insdict['precision'] = [prec]\n                insdict['recall'] = [rec]\n                adv_res[akey] = insdict\n        \n        if adv not in resdict:\n            resdict[adv] = adv_res\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COLNAMES = ['name','cat_prec', 'cat_rec', 'sv_prec', 'sv_rec']\nto_df = pd.DataFrame(columns = COLNAMES)\ncat_dict = resdict['cat']\nsv_dict = resdict['sub_val']\ncats = sorted(list(cat_dict.keys()))\nfor cat in cats:\n    c_prec = np.mean(cat_dict[cat]['precision'])\n    c_rec = np.mean(cat_dict[cat]['recall'])\n    sv_prec = np.mean(sv_dict[cat]['precision'])\n    sv_rec = np.mean(sv_dict[cat]['recall'])\n    row = [cat, c_prec,c_rec,sv_prec,sv_rec]\n    \n    to_df.loc[len(to_df)] = row\n    \nto_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the (mean) results\nalldf.to_csv('all_k_results2.csv')\ntestd = alldf.groupby(['class']).mean()\ntestdf.to_csv('all_k_grouped.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_results = pd.read_csv('../input/privbert-data/all_k_prec_recall.csv')\nall_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subvals = all_results[list(all_results.columns[-4:])].values\nfor i in range(4):\n   # print(subvals[:,i][:10])\n    print(np.mean(subvals[:,i][:10]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subvals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error Analysis\n\nThe following code is used to analyze the prediction results in more depth, i.e. I have a look at the numper of True Positivesn False Positives, True Negatives and False negatives.","metadata":{}},{"cell_type":"code","source":"def find_differences(predicts, truths,treshold = 0.6):\n  \"\"\"\n  Function that looks compares predictions and true labels, stroes the difference between \n  the predictions and the treshold and counts the number of TP, FP, TN, FN.\n  \n  \"\"\"\n    \n  # set variables\n  TP = 0\n  FP = 0\n  TN = 0\n  FN = 0\n\n  TP_L = []\n  FP_L = []\n  TN_L = []\n  FN_L = []\n\n  mistake_set = set()\n\n  #go over each prediction \n  for i in range(len(predicts)):\n    predict_array = predicts[i]\n    truths_array = truths[i]\n    #look at each labvel individually\n    for j in range(len(predict_array)):\n      label_score = predict_array[j]\n    \n      # set the label of the predicion\n      if label_score < treshold:\n        bool_score = 0\n      else:\n        bool_score = 1\n      diff_from_treshold = abs(treshold - label_score) # save the difference between the prediction and boolean threshold\n      truth_score = truths_array[j]\n\n      # count TN, TP, FN, FP \n      if bool_score == 0 and truth_score == 0:\n        TN += 1\n        TN_L.append(diff_from_treshold)\n      \n      elif bool_score == 1 and truth_score == 1:\n        TP += 1\n        TP_L.append(diff_from_treshold)\n\n      elif bool_score == 0 and truth_score == 1:\n        FN += 1\n        FN_L.append(diff_from_treshold)\n        mistake_set.add(i) # if it is not the same the whole index is added tot the mistake set\n      \n      else:\n        FP += 1\n        FP_L.append(diff_from_treshold)\n        mistake_set.add(i) # if it is not the same the whole index is added tot the mistake set\n    \n  mistake_list = list(mistake_set)\n  return ((TP,TN,FP,FN),(TP_L,TN_L,FP_L,FN_L),mistake_set)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In the paper I had a closer look at the results for the fifth (i.e. k4) set\nDATAPATH = '../input/privbert-data/predictions_k4.csv'\nresult_df = pd.read_csv(DATAPATH)\nprednames = result_df.columns[2:12]\ntruenames = result_df.columns[12:]\nprint(result_df['advice_name'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similar function as the one above when analyzing the mean scores, however, here we only look at 2 advice names\n# 'cat' and 'sub_val' ('sub_val' in reality is 'suball_val', but I made a mistake in naming)\nfrom sklearn.metrics import f1_score\ncatlist = []\nsvlist = []\ncatf1 = []\nsvvf1 = []\nfor i in range(5):\n    DATAPATH = '../input/privbert-data/predictions_k{}.csv'.format(i)\n    result_df = pd.read_csv(DATAPATH)\n    catdf = result_df[result_df['advice_name'] == 'cat']\n    subvaldf = result_df[result_df['advice_name'] == 'sub_val']\n    \n    cat_preds = catdf[prednames].values\n    cat_labels = catdf[truenames].values\n    cat_bools = [pred > 0.6 for pred in cat_preds] # cat the boolean predictions for the 'cat' / 'base' advice layer\n    \n    sv_preds = subvaldf[prednames].values\n    sv_labels = subvaldf[truenames].values\n    sv_bools = [pred > 0.6 for pred in sv_preds] # cat the boolean predictions for the 'sub_val'= 'suball_val' advice layer\n    \n    cat_diffs = find_differences(cat_preds, cat_labels)\n    sv_diffs = find_differences(sv_preds, sv_labels)\n    \n    #print(cat_preds[0],type(cat_labels[0]))\n    \n    cf1 = f1_score(cat_labels,cat_bools, average = 'micro') # caclualte scores\n    svf1 = f1_score(sv_labels,sv_bools, average = 'micro')\n    \n    catf1.append(cf1) # add score\n    svvf1.append(svf1)\n    \n    catlist.append(cat_diffs)\n    svlist.append(sv_diffs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the number of TP, TN, FP, FN for the test sets\ncatallpr = [0] *4 # for the categories\nsvallpr = [0] * 4 # for the suball_val\nfor i in range(5):\n    pr = catlist[i][0]\n    rp = svlist[i][0]\n    for j,p in enumerate(pr):\n        if catallpr[j]:\n            catallpr[j].append(p)\n        else:\n            catallpr[j] = [p]\n        \n    for k,r in enumerate(rp):\n        if svallpr[k]:\n            svallpr[k].append(r)\n        else:\n            svallpr[k] = [r]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catallpr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svallpr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(svallpr) # example of results, first row is all the TP \n# second row is TN,\n# third row is FP\n# fourth row is FN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate means scores\nkmean = []\nsvmean = []\nfor i in range(4):\n    ksum = np.mean(catallpr[i])\n    svsum = np.mean(svallpr[i])\n    kmean.append(ksum)\n    svmean.append(svsum)\n    print(ksum,svsum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Below are extra functions that were used more on the fly, for example for when choosing three example segments","metadata":{}},{"cell_type":"code","source":"k4_cat = result_df[result_df['advice_name'] == 'cat']\nk4_sv = result_df[result_df['advice_name'] == 'sub_val']\n\nk4_preds_cat = np.array(list(k4_cat[prednames].values))\nk4_bools_cat = [p > 0.6 for p in k4_preds_cat]\nk4_true_cat = np.array(list(k4_cat[truenames].values))\n\nk4_preds_sv = np.array(list(k4_sv[prednames].values))\nk4_bools_sv = [p > 0.6 for p in k4_preds_sv]\nk4_true_sv = np.array(list(k4_sv[truenames].values))\n\nres_cat = find_differences(k4_bools_cat,k4_true_cat)\nres_sv = find_differences(k4_bools_sv,k4_true_sv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uncats = [i for i in res_cat[2] if i not in res_sv[2]]\nunsv = [i for i in res_sv[2] if i not in res_cat[2]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# uncats and unsv stand for unique-cats and unique-sv. That is unique mistakes, so a unique-cat is a mistake that was only\n# a mistake in that advice layer, but not in the next\n\nuncatbools = np.array(cat_bools)[uncats]\nunsvbools = np.array(sv_bools)[list(unsv)]\nuncatlabels = np.array(cat_labels)[uncats]\nunsvlabels = np.array(sv_labels)[list(unsv)]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catsumlab = []\nfor i in range(len(uncatlabels)):\n    catsumlab.append(sum(uncatlabels[i]))\n    \ncatsumbool = []\nfor i in range(len(uncatbools)):\n    catsumbool.append(sum(uncatbools[i]))\n\nprint(catsumlab)\nprint(catsumbool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sumlabels = []\nfor i in range(len(unsvlabels)):\n    sumlabels.append(sum(unsvlabels[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sumbools = []\nfor i in range(len(unsvbools)):\n    sumbools.append(sum(unsvbools[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stayedsame = [a for a in res_cat[2] if a not in uncats]\nprint(len(stayedsame))\ncatsameb = np.array(cat_bools)[stayedsame]\ncatsamel = np.array(cat_labels)[stayedsame]\nsvsameb = np.array(sv_bools)[stayedsame]\nsvsamel = np.array(sv_labels)[stayedsame]\n\ncatsumlabs = []\nfor i in range(len(catsamel)):\n    catsumlabs.append(sum(catsamel[i]))\n    \ncatsumbools = []\nfor i in range(len(catsameb)):\n    catsumbools.append(sum(catsameb[i]))\n    \nsvsumlabs = []\nfor i in range(len(svsamel)):\n    svsumlabs.append(sum(svsamel[i]))\n    \nsvsumbools = []\nfor i in range(len(svsameb)):\n    svsumbools.append(sum(svsameb[i]))\n\ndiffarr_cat = np.array(catsumlabs) - np.array(catsumbools)\ndiffarr_lab = np.array(svsumlabs) - np.array(svsumbools)\ntotdiff = diffarr_cat + diffarr_lab\nprint(totdiff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slist = []\nfor i in range(10):\n    aa = sum(catsameb[:,i]) - sum(catsamel[:,i])\n    print(aa)\n    slist.append(aa)\n\nprint()\nprint(sum(slist))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fplist = []\nfnlist = []\nfor i in range(10):\n    aal =svsamel[:,i] - svsameb[:,i]\n    print(\"FP count for {} : {}\".format(cat_names[i],np.count_nonzero(aal == -1)))\n    print(\"FN count for {} : {}\".format(cat_names[i],np.count_nonzero(aal == 1)))\n    #print(sum(aal))\n    fplist.append(np.count_nonzero(aal == -1))\n    fnlist.append(np.count_nonzero(aal == 1))\n\nprint(\"total false positives: \",sum(fplist))\nprint(\"total false negatives: \",sum(fnlist))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_fp = fplist\ncat_fn = fnlist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fp_inc = [fp / cat_fp[i] for i,fp in enumerate(fplist) if cat_fp[i]]\nfn_inc = [fn / cat_fn[i] for i,fn in enumerate(fnlist) if cat_fn[i]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fp_inc = [1] + fp_inc\nfp_inc = fp_inc[:2] + [1] + fp_inc[2:]\nfp_inc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fn_inc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_names[9]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sv_fpperc = [fp / lcount[i] for i,fp in enumerate(fplist)]\nsfnperc = [fn / lcount[i] for i,fn in enumerate(fnlist)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_fpperc = [fp / lcount[i] for i,fp in enumerate(fplist)]\ncat_fnperc = [fn / lcount[i] for i,fn in enumerate(fnlist)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff_fp = np.array(cat_fpperc) - np.array(sv_fpperc)\ndiff_fn = np.array(cat_fnperc) - np.array(sv_fnperc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slist = []\nsslist = []\nfor i in range(10):\n    aal = unsvlabels[:,i] - unsvbools[:,i]\n    print(\"FP count for {} : {}\".format(cat_names[i],np.count_nonzero(aal == -1)))\n    print(\"FN count for {} : {}\".format(cat_names[i],np.count_nonzero(aal == 1)))\n    #print(sum(aal))\n    slist.append(np.count_nonzero(aal == -1))\n    sslist.append(np.count_nonzero(aal == 1))\n\nprint(sum(slist))\nprint(sum(sslist))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catsameb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example of poorly calculated one\nsumbools.index(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum([True,False,True])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# poorer :\nmadeworse = list(unsv)[17]\n# better:\nmadebetter = uncats[0]\n#same :\nmadesame = stayedsame[7]\nprint(madebetter,madeworse,madesame)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"truenames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.iloc[madeworse]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(onlyfortexts.segments_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"made_df = onlyfortexts.df[onlyfortexts.df['segment_text'] == texts[madeworse]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"made_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[madeworse]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testf = pd.read_csv('../input/privbert-data/op115_data/op115_test_k4.csv')\nonlyfortexts = Op115OneHots(testf)\nonlyfortexts.go2(majority = True, class_tup = uniques)\n\nt_catsub,t_catval,t_subval,train_cats,t_subs,t_vals,texts = onlyfortexts.new_onehots()\nprint(len(texts))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lcount = []\nfor i in range(10):\n    sumcounts = sum(np.array(train_cats)[:,i])\n    print(\"label count for {} : {}\".format(cat_names[i],sumcounts))\n    lcount.append(sumcounts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k4_sv.iloc[madeworse]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelz_c = []\npredz_c = []\nlabelz_sv = []\npredz_sv = []\nsubadvices = []\nvaladvices = []\ntextz = []\n\nfor index in [madeworse]:\n    catsub = catdf.iloc[index]\n    svsub = subvaldf.iloc[index]\n    subsub = subdf.iloc[index]\n    valsub = valdf.iloc[index]\n    text = texts[index]\n\n    \n    cat_preds = catsub[prednames].values\n    cat_labels = catsub[truenames].values\n    \n    sv_preds = svsub[prednames].values\n    sv_labels = svsub[truenames].values\n    \n    sub_preds = subsub[prednames].values\n    val_preds = valsub[prednames].values\n    \n    sub_adv = sub_preds - cat_preds\n    val_adv = val_preds - cat_preds\n    \n    subadvices.append(sub_adv)\n    valadvices.append(val_adv)\n    \n    labelz_c.append(cat_labels)\n    labelz_sv.append(sv_labels)\n    \n    predz_c.append(cat_preds)\n    predz_sv.append(sv_preds)\n    \n    textz.append(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}