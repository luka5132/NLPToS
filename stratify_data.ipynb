{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Processing the data\n\nIn this notebook parts of the data that was used for my capstone is processed. Below the codes can be found to:\n1. Do initial processing of raw OPP-115 data\n2. Divide data into 5 fold training / validation and testing set\n3. Locate the best parameters for each class with the grid search that was done in the 'pribert_full_model.ipynb' file","metadata":{}},{"cell_type":"markdown","source":"# Dividing the data\nbelow it is shown how the *'op115_processed.csv'* data is used in to create 5 different sets.","metadata":{}},{"cell_type":"code","source":"!pip install verstack\nimport verstack\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '../input/privbert-data/op115_data/op115_train_k0.csv'\nVAL_PATH = '../input/privbert-data/op115_data/op115_val_k0.csv'\nTEST_PATH = '../input/privbert-data/op115_data/op115_test_k0.csv'\nALL_PATH = '../input/privbert-data/op115_processed.csv'\nop115_train = pd.read_csv(TRAIN_PATH)\nop115_val = pd.read_csv(VAL_PATH)\nop115_test = pd.read_csv(TEST_PATH)\nop115_all = pd.read_csv(ALL_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(op115_train.policy_uid.unique())\nprint(op115_val.policy_uid.unique())\nprint(op115_test.policy_uid.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/privbert-data/data_processing.py\", dst = \"../working/data_processing.py\")\ncopyfile(src = \"../input/privbert-data/pytorch_classifier.py\", dst = \"../working/pytorch_classifier.py\")\ncopyfile(src = \"../input/privbert-data/hierarchical_data.py\", dst = \"../working/hierarchical_data.py\")\ncopyfile(src = \"../input/privbert-data/privbert_gridsearches3.csv\", dst = \"../working/privbert_gridsearches.csv\")\ncopyfile(src = \"../input/privbert-data/tresholds.csv\", dst = \"../working/treshold.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from data_processing import Op115OneHots\nALL_POLS = '../input/privbert-data/op115_processed.csv'\nall_pols_df = pd.read_csv(ALL_POLS)\npoll_uids = sorted(all_pols_df.policy_uid.unique())\nlabels_per_pol = []\n\nop115_all_c = Op115OneHots(all_pols_df)\nop115_all_c.go2(majority = True)\nuniques = op115_all_c.return_oh_names()\ncatsub_index, catval_index, subval_index, inds = op115_all_c.len_onehots()\n\ncatsub,catval,subval,cats,subs,vals,my_texts = op115_all_c.new_onehots()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catarr = np.array(cats)\nlabelsupport = []\nfor i in range(10):\n    labelsupport.append(sum(catarr[:,i]))\n\nprint(labelsupport)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_arry = np.array(cats)\nindexes = []\nfor i in range(10):\n    print(sum(cat_arry[:,i]))\n    colrow= cat_arry[:,i]\n    inds = np.where(colrow == 1)\n    indexes.append(inds)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policies = []\npolsegs = op115_all_c.pol_seg()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    pol_uids = []\n    for ind in indexes[i][0].astype(int):\n        pol_uids.append(polsegs[ind][0])\n    policies.append(pol_uids)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = 0\nspecials = [0,2,6,8]\nspecial_pols = []\nfor special in specials:\n    print(set(policies[special]))\n    special_pols = special_pols + policies[special]\n\nspecial_pols = list(set(special_pols))\nprint(special_pols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_val_inds, test_inds = train_test_split(special_pols, test_size = 0.35)\ntrain_inds, val_inds = train_test_split(train_val_inds, test_size = 0.2)\nprint(len(train_inds),len(val_inds),len(test_inds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport errno\nfilename = \"./op115_data/\"\nif not os.path.exists(os.path.dirname(filename)):\n    try:\n        os.makedirs(os.path.dirname(filename))\n    except OSError as exc: # Guard against race condition\n        if exc.errno != errno.EEXIST:\n            raise\n\n\nkfold = 5\nall_poluids = all_pols_df['policy_uid'].unique()\nwithout_special = [poluid for poluid in all_poluids if poluid not in special_pols]\nfor i in range(kfold):\n    train_val_inds, test_inds = train_test_split(special_pols, test_size = 0.35)\n    train_inds, val_inds = train_test_split(train_val_inds, test_size = 0.2)\n    \n    train_val_pols, test_pols = train_test_split(without_special, test_size = 0.24)\n    train_pols, val_pols = train_test_split(train_val_pols, test_size = 0.16)\n    \n    train_pols = train_pols + train_inds\n    val_pols = val_pols + val_inds\n    test_pols = test_pols + test_inds\n    \n    train_df = all_pols_df[all_pols_df['policy_uid'].isin(train_pols)]\n    val_df = all_pols_df[all_pols_df['policy_uid'].isin(val_pols)]\n    test_df = all_pols_df[all_pols_df['policy_uid'].isin(test_pols)]\n    \n    train_df.to_csv('./op115_data/op115_train_k{}.csv'.format(i))\n    val_df.to_csv('./op115_data/op115_val_k{}.csv'.format(i))\n    test_df.to_csv('./op115_data/op115_test_k{}.csv'.format(i))\n\n    print(len(train_pols),len(val_pols),len(test_pols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Picking optimal paramters\nBelow it is shown how the optimal parameters for each class were picked using the gridsearch","metadata":{}},{"cell_type":"code","source":"#load data\nimport pandas as pd\nimport numpy as np\nvertical_stack = pd.read_csv('data/advice_tresholds.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the category / advice models names\ncat_names = list(vertical_stack['class'].unique())\nadvice_models = vertical_stackf.columns[-8:]\nfinal_tresholds = list(vertical_stack['final_treshold'].unique()) # all best paramters must have the same final treshold\nprint(advice_models)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a dataframe with the best scores per final_treshold per category\nmax_df = pd.DataFrame(columns = vertical_stack.columns)\nfor fn in final_tresholds:\n    fn_df = vertical_stack[vertical_stack['final_treshold'] == fn] #subdf with only that final treshold\n    for cn in cat_names:\n        subdf = fn_df[fn_df['class'] == cn] # subdf with only respective class\n        subdf_advice = subdf[advice_models] # only take the advice model values\n        max_inds = subdf_advice.idxmax(axis = 0).tolist() #indexes with maximum values\n        donelist = []\n        for ind in max_inds:\n            if ind not in donelist: #dont want duplcates\n                row = list(subdf.loc[ind].values) #create a row with maximum values to add to the df\n\n                max_df.loc[len(max_df)] = row\n                donelist.append(ind)\nprint(len(max_df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_inds(adf):\n    \"\"\"\n    This function takes a df and returns the indexes from when each new class starts\n    \"\"\"\n    inds_classes = []\n    classes =  adf['class'].to_list()\n    for i in range(len(classes)-1):\n        if classes[i] != classes[i+1]:\n            inds_classes.append(i+1)\n    inds_classes = [0] + inds_classes +[len(adf)]\n    return inds_classes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create initial output list, dimensions are 8x6. 8 for each advice layer combination and 6 for each final threshold\nscores_per_val = [[0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0]]\n\nindex_per_val = [[0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from data_processing import Op115OneHots\n\n#To test how well it did I wanted to not only look at the scores per class but also the support for the respective class\n#for that I've taken the first validation set as an example\n\nVAL_PATH = 'data/valk0.csv'\nval_df = pd.read_csv(VAL_PATH)\nop115_v_c = Op115OneHots(val_df)\nop115_v_c.go2(majority = True)\nv_catsub,v_catval,v_subval,v_cats,v_subs,v_vals,v_my_texts = op115_v_c.new_onehots()\nnpcats = np.array(v_cats)\nn_classes = []\nfor i in range(10):\n    n_classes.append(sum(npcats[:,i]))\nprint(n_classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the maxium scores into the respecitve outputlist\npast_lenghts = 0\nmax_indlist = []\nklist = []\nfor k,fn in enumerate(final_tresholds):\n    fn_subdf = max_df[max_df['final_treshold'] == fn] # subdf again\n    fn_subdf = fn_subdf.reset_index()\n    inds_classes = return_inds(fn_subdf)\n    max_df_scores = fn_subdf[advice_models].values\n    for i in range(len(inds_classes)-1): # here we use the indexes\n        s = inds_classes[i]\n        n = inds_classes[i+1]\n        for j in range(s,n): \n            max_vals = max_df_scores[j,:] * n_classes[i] # multiply percentage scores correct with total amount of support\n            max_df_scores[j,:] = max_vals # set the maxium scores\n \n        for m in range(8): # iterate over each advice set\n            cat_vals = max_df_scores[s:n]\n            max_val = max(cat_vals[:,m])\n\n            max_index = list(cat_vals[:,m]).index(max_val) + s + past_lenghts\n            max_indlist.append(max_index)\n            klist.append(k)\n            #input the scores per k\n            if scores_per_val[k][m]:\n                scores_per_val[k][m].append(max_val)\n                index_per_val[k][m].append(max_index)\n            else:\n                scores_per_val[k][m] = [max_val]\n                index_per_val[k][m] = [max_index]\n    past_lenghts += len(max_df_scores)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Here we take the scores per k and put them together\nmax_per_val = [[0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0],\n                  [0,0,0,0,0,0,0,0]]\n\nfor k,fn in enumerate(final_tresholds):\n    tr_scores = scores_per_val[k]\n    for i,adv in enumerate(tr_scores):\n        scores = np.mean(adv)\n        max_per_val[k][i] = scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using the combined scores we can now pick the best one\nmaxmaxval = []\nmaxmaxind = []\nfor scorelist in max_per_val:\n    maxval = max(scorelist)\n    catval = scorelist[0]\n    maxind = (scorelist.index(maxval))\n    maxmaxval.append(maxval)\n    maxmaxind.append(maxind)\n    print(maxind,maxval,catval)\n    \nbestval = max(maxmaxval)\nbestind = maxmaxval.index(bestval)\nbesterind = maxmaxind[bestind]\nbesttup = (bestind,besterind)\nbestvalues = index_per_val[bestind][besterind]\n\n# a subdf that contains the best parameters\nbest_df = max_df.loc[bestvalues]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output the df as a json file that can be used \nimport json\n\nbest_param_dict = {}\nfor aclass in best_df['class'].values:\n    param_dict = {}\n    classrow = best_df[best_df['class'] == aclass]\n    parnames = classrow.columns[2:8]\n    tresholds = classrow[parnames].values[0]\n    for i,parname in enumerate(parnames):\n        param_dict[parname] = [tresholds[i]]\n    best_param_dict[aclass] = param_dict\n    print(len(tresholds),tresholds)\n    \nwith open('advice_parameters.json', 'w') as outfile:\n    json.dump(best_param_dict, outfile)\n","metadata":{},"execution_count":null,"outputs":[]}]}