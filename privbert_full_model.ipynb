{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Full Hierarchichal System\n\nIn this notebook one can find the code that is used for the full hierarchichal classifier as described in my capstone:\nhttps://github.com/luka5132/NLPToS\nIt uses the trained classification models that were created using the privbert.ipynb notebook (https://www.kaggle.com/lukasbusch/privbert) and the training data that can be found in privbert-data.\n\nIn this notebook I:\n- Define the 'hierarchical classification' function which leverages all the classification models to perform a multi level classification\n- Perfrom a grid search to find the optimal parameters for the advice function as well as the best 'candidate threshold' and 'final treshold'","metadata":{}},{"cell_type":"code","source":"#load libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import *\nfrom tqdm import tqdm, trange \nfrom ast import literal_eval\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T15:42:10.48243Z","iopub.execute_input":"2021-05-21T15:42:10.482782Z","iopub.status.idle":"2021-05-21T15:42:20.908635Z","shell.execute_reply.started":"2021-05-21T15:42:10.482751Z","shell.execute_reply":"2021-05-21T15:42:20.907352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set device\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:42:20.912559Z","iopub.execute_input":"2021-05-21T15:42:20.912949Z","iopub.status.idle":"2021-05-21T15:42:27.806219Z","shell.execute_reply.started":"2021-05-21T15:42:20.912917Z","shell.execute_reply":"2021-05-21T15:42:27.804933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/privbert-data/data_processing.py\", dst = \"../working/data_processing.py\")\ncopyfile(src = \"../input/privbert-data/pytorch_classifier.py\", dst = \"../working/pytorch_classifier.py\")\ncopyfile(src = \"../input/privbert-data/hierarchical_data.py\", dst = \"../working/hierarchical_data.py\")\ncopyfile(src = \"../input/privbert-data/privbert_gridsearches3.csv\", dst = \"../working/privbert_gridsearches.csv\")\n#copyfile(src = \"../input/privbert-data/tresholds.csv\", dst = \"../working/treshold.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:42:27.808519Z","iopub.execute_input":"2021-05-21T15:42:27.809027Z","iopub.status.idle":"2021-05-21T15:42:27.82777Z","shell.execute_reply.started":"2021-05-21T15:42:27.808967Z","shell.execute_reply":"2021-05-21T15:42:27.826224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from data_processing import Op115OneHots\n\n#Load data, for more information on Op115OneHots please look at the notebook file with the same name\nVAL_PATH_K1 = '../input/privbert-data/op115_data/op115_val_k0.csv'\nTEST_PATH_K1 = '../input/privbert-data/op115_data/op115_test_k0.csv'\nTRAIN_PATH_K1 = '../input/privbert-data/op115_data/op115_train_k0.csv'\nALL_PATH = '../input/privbert-data/op115_processed.csv'\nop115_val = pd.read_csv(VAL_PATH_K1)\nop115_test= pd.read_csv(TEST_PATH_K1)\nop115_train= pd.read_csv(TRAIN_PATH_K1)\nop115_all = pd.read_csv(ALL_PATH)\n\nop115_all_c = Op115OneHots(op115_all)\nop115_all_c.go2()\n\nuniques = op115_all_c.return_oh_names() # need this to make sure all one-hot vectors have the same dimension\ncatsub_index, catval_index, subval_index, inds = op115_all_c.len_onehots()\n\ncat_names = sorted(op115_all_c.unique_cats)\n\n#load validation data\nop115_v_c = Op115OneHots(op115_val)\nop115_v_c.go2(majority = True, class_tup = uniques)\n\n#load test data\nop115_t_c = Op115OneHots(op115_test)\nop115_t_c.go2(majority = True, class_tup = uniques)\n\nv_catsub,v_catval,v_subval,v_cats,v_subs,v_vals,v_my_texts = op115_v_c.new_onehots()\nt_catsub,t_catval,t_subval,t_cats,t_subs,t_vals,t_my_texts = op115_t_c.new_onehots()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:42:27.831295Z","iopub.execute_input":"2021-05-21T15:42:27.831633Z","iopub.status.idle":"2021-05-21T15:42:35.458683Z","shell.execute_reply.started":"2021-05-21T15:42:27.831603Z","shell.execute_reply":"2021-05-21T15:42:35.457676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the indexes for the suball classifier. This way one knows which indexes of the label belong to which classifier\nindskeys = list(inds.keys())\nlen2 = []\nfor ba in indskeys:\n    if len(ba) == 2 and ba[0] in cat_names:\n        len2.append(ba)\n\nlen2 = sorted(len2)\n\nsub_indexes = {}\nfor atup in len2:\n    cat = atup[0]\n    sub = atup[1]\n    cat_ind = inds[cat]\n    sub_tup = (cat_ind,cat,sub)\n    if cat in sub_indexes:\n        sub_indexes[cat].append(inds[sub_tup])\n    else:\n        sub_indexes[cat] = [inds[sub_tup]] \n    \nprint(sub_indexes)\n                  ","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:42:46.92723Z","iopub.execute_input":"2021-05-21T15:42:46.927692Z","iopub.status.idle":"2021-05-21T15:42:46.937717Z","shell.execute_reply.started":"2021-05-21T15:42:46.92766Z","shell.execute_reply":"2021-05-21T15:42:46.936394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = 'bert-base-uncased'\ntryout = BertClassification(gpu = False)\ntryout.test = True\ntryout.train = False\ntryout.init_tokenizer(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:43:47.125843Z","iopub.execute_input":"2021-05-21T15:43:47.12619Z","iopub.status.idle":"2021-05-21T15:43:50.932514Z","shell.execute_reply.started":"2021-05-21T15:43:47.12616Z","shell.execute_reply":"2021-05-21T15:43:50.931465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tryout.tokenizer.tokenize(v_my_texts[11])","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:44:38.917793Z","iopub.execute_input":"2021-05-21T15:44:38.918158Z","iopub.status.idle":"2021-05-21T15:44:38.934426Z","shell.execute_reply.started":"2021-05-21T15:44:38.918113Z","shell.execute_reply":"2021-05-21T15:44:38.933228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_classifier import BertClassification\n\ndef load_test_data(tokenizer, texts, batch_size, max_length, labels = None):\n    \"\"\"\n    Quick function that loads data into dataloaders. Partly uses the 'BertClassification' as defined in\n    pytorch_classifier.py\n    \"\"\"\n    tryout = BertClassification(gpu = False)\n    tryout.test = True\n    tryout.train = False\n    tryout.init_tokenizer(tokenizer)\n    if labels:\n        tryout.init_data(texts,labels)\n        return tryout.encode_texts(max_length,batchsize = batch_size) # train_data, val_data\n    else:\n        tryout.input_texts(texts)\n        return tryout.encode_texts(max_length,batchsize = batch_size, with_labels = False) # train_data, val_data\n\n   ","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:43:38.11148Z","iopub.execute_input":"2021-05-21T15:43:38.111865Z","iopub.status.idle":"2021-05-21T15:43:38.126225Z","shell.execute_reply.started":"2021-05-21T15:43:38.111804Z","shell.execute_reply":"2021-05-21T15:43:38.125062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_test(model, test_dataloader, with_labels = False):\n    \"\"\"\n    Perform a test on a test_dataloader, has the option to add labels as well, that way it returns not only the predictions\n    but the respective true labels as well.\n    \n    \"\"\" \n    # Put model in evaluation mode to evaluate loss on the validation set\n    model.eval()\n        \n    #track variables\n    if with_labels:\n        logit_preds,true_labels,pred_labels = [],[],[]\n    else:\n        logit_preds,pred_labels = [],[]\n        \n    # Predict\n    for i, batch in enumerate(test_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        if with_labels:\n            b_input_ids, b_input_mask, b_labels, b_token_types = batch\n        else:\n            b_input_ids, b_input_mask, b_token_types = batch\n        with torch.no_grad():\n            # Forward pass\n            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n            b_logit_pred = outs[0]\n            pred_label = torch.sigmoid(b_logit_pred)\n        \n            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n            pred_label = pred_label.to('cpu').numpy()\n            \n            if with_labels:\n                b_labels = b_labels.to('cpu').numpy()\n        \n        logit_preds.append(b_logit_pred)\n        pred_labels.append(pred_label)\n        if with_labels:\n            true_labels.append(b_labels)\n            \n        \n    # Flatten outputs\n    pred_labels = [item for sublist in pred_labels for item in sublist]\n\n    if with_labels:\n        true_labels = [item for sublist in true_labels for item in sublist] # Flatten outputs\n        true_bools = [tl==1 for tl in true_labels]\n        return (pred_labels, true_bools)\n    else:\n        return pred_labels\n        ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T20:58:49.797605Z","iopub.execute_input":"2021-05-20T20:58:49.797891Z","iopub.status.idle":"2021-05-20T20:58:49.811105Z","shell.execute_reply.started":"2021-05-20T20:58:49.797865Z","shell.execute_reply":"2021-05-20T20:58:49.810032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set the variables for a gridsearch","metadata":{}},{"cell_type":"code","source":"import os\nML_AND_BS = ['max_length','batch_size']\nSAVEFILE = 'privbert_gridsearches.csv'\ngridsearch_results = pd.read_csv(SAVEFILE)\ngr_cols = list(gridsearch_results.columns)\nparam_df = gridsearch_results[ML_AND_BS]\nmodel_names = gr_cols[5:]\n\n\nmodel_parameter = {} # get the best parameters for each model \n# important when loading the dataloader as we want the maximum length and batch size to be the one used for training the model\nfor model_name in model_names:\n    col_results = gridsearch_results[model_name].to_list()\n    best_res = col_results.index(max(col_results))\n    best_param = param_df.iloc[best_res].to_list()\n    model_parameter[model_name] = best_param\n\n#the models are saved in such a way that when a '/' appeared in the name that was seen as a subfolder.\n#I have taken the files out of the subfolder but the name of the model is thus without the '/'.\n#The model_pahts dictionary takes the name of a model and return the correct path to the location of that model.\nmodels_dir = '../input/privbert-models/trained_models'\nmodels_paths = {}\nfor model_name in model_names:\n    if '/' in model_name:\n        ml_name = model_name.split('/')[0] #Because of the way the files were zipped, the / was seen as a subfolder, this was removed. Thus the path is without /...\n        models_paths[model_name] = os.path.join(models_dir,ml_name)\n    else:\n        models_paths[model_name] = os.path.join(models_dir,model_name)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T20:58:49.812967Z","iopub.execute_input":"2021-05-20T20:58:49.813304Z","iopub.status.idle":"2021-05-20T20:58:49.837218Z","shell.execute_reply.started":"2021-05-20T20:58:49.813275Z","shell.execute_reply":"2021-05-20T20:58:49.836301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_parameter = {} # get the best parameters for each model \n# important when loading the dataloader as we want the maximum length and batch size to be the one used for training the model\nfor model_name in model_names:\n    col_results = gridsearch_results[model_name].to_list()\n    best_res = col_results.index(max(col_results))\n    best_param = gridsearch_results[gr_cols[:5]].iloc[best_res].to_list()\n    model_parameter[model_name] = best_param","metadata":{"execution":{"iopub.status.busy":"2021-05-20T21:01:20.992922Z","iopub.execute_input":"2021-05-20T21:01:20.993374Z","iopub.status.idle":"2021-05-20T21:01:21.017123Z","shell.execute_reply.started":"2021-05-20T21:01:20.993319Z","shell.execute_reply":"2021-05-20T21:01:21.015911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(gr_cols[5:]))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T21:35:52.911528Z","iopub.execute_input":"2021-05-20T21:35:52.911867Z","iopub.status.idle":"2021-05-20T21:35:52.916754Z","shell.execute_reply.started":"2021-05-20T21:35:52.911839Z","shell.execute_reply":"2021-05-20T21:35:52.91574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for mp in model_parameter:\n    print(\"for {}: \\n {} \\n\".format(mp,model_parameter[mp]))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T21:20:25.410293Z","iopub.execute_input":"2021-05-20T21:20:25.410641Z","iopub.status.idle":"2021-05-20T21:20:25.416395Z","shell.execute_reply.started":"2021-05-20T21:20:25.410615Z","shell.execute_reply":"2021-05-20T21:20:25.41546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### module_sizes = {} # dictionary that stores the dimension of each classifier. \n# For example: Categories : 10\n#              Subcategories : 36\nfor amodel in model_names:\n    if amodel.startswith('cs'):\n        strip_cs = amodel.split('_')[1]\n        output_size = catsub_index[strip_cs]\n        module_sizes[amodel] = output_size\n    elif amodel.startswith('cv'):\n        strip_cv = amodel.split('_')[1]\n        output_size = catval_index[strip_cv]\n        module_sizes[amodel] = output_size\n    elif amodel == 'Subcategories':\n        module_sizes[amodel] = 36\n    else:\n        module_sizes[amodel] = 10\n        \nprint(module_sizes)","metadata":{}},{"cell_type":"code","source":"# Set parameters for gridsearch\nPARAMS = {  'label_treshold' : [0.4,0.5,0.6,0.7,0.8,0.9],\n            'advice_val' : [0.05,0.1,0.15,0.2,0.25],\n            'subseq_val' : [0.02,0.04,0.06,0.08,0.10],\n            'negative_advice' : [-0.05,-0.1,-0.15,-0.2,-0.25,-0.3,-0.35,-0.4]}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hierarchical_data import HierarchicalData #for more information please see the respective notebook\nfrom keras import backend as K\n\ndef hierarchical_classification(tokname, texts, params_per_model, model_to_path, sub_index, candidate_treshold, \n                                model_size_dict = module_sizes, cat_names = cat_names, gridsearch = False, params = PARAMS):\n    \"\"\"\n    Function that performs the full hierarchical classification process on a list of texts/segments. \n    Requires a tokenizer and a dictionary that contains the information of how texts should be turned into a dataloader.\n    It passes this information to the 'load_test_data' function.\n    \n    Using the model_to_path dictionary the function is able to load the respective classifier when needed.\n    \n    The sub_index is a dictionary that links the index of the suball classification model to the respective categories\n    \n    The candidate_treshold determines which prediction scores require further evaluation.\n    \n    \"\"\"\n    \n    pred_dict = {}\n    \n    #Start by loading the Categories and Subcategories predictions into the HierarchicalData class\n    category_path = model_to_path['Categories']\n    subcategory_path = model_to_path['Subcategories']\n    cat_ml, cat_bs = params_per_model['Categories']\n    sub_ml, sub_bs = params_per_model['Subcategories']\n    \n    model = BertForSequenceClassification.from_pretrained(category_path, num_labels=10)\n    model.cuda()\n    \n    test_dataloader = load_test_data(tokname, texts, cat_bs, cat_ml)\n    cat_preds= perform_test(model, test_dataloader) # predictions for categories\n    \n    #initiate the advice class and store the predictions\n    advice_class = HierarchicalData(candidate_treshold)\n    advice_class.set_parameters(params)\n    advice_class.set_variables(cat_names)\n    advice_class.read_cat_predictions(np.array(cat_preds))\n    advice_class.define_candidates(candidate_treshold) # determine candidates\n    \n    # clear memory\n    K.clear_session()\n    torch.cuda.empty_cache()\n    \n    model = BertForSequenceClassification.from_pretrained(subcategory_path, num_labels=36)\n    model.cuda()\n    \n    test_dataloader = load_test_data(tokname, texts, sub_bs, sub_ml)\n    sub_preds = perform_test(model, test_dataloader) # get predictions for the subcategory layer\n    \n    pred_dict['suball'] = sub_preds\n    \n    advice_class.read_sub_predictions(np.array(sub_preds)) # save predictions\n    \n    K.clear_session()\n    torch.cuda.empty_cache()\n    \n    # if we perform a gridsearch we go through all the paramters and store the advice given with the 'suball' layer\n    if gridsearch:\n        label_treshold = params['label_treshold']\n        advice_val = params['advice_val']\n        subseq_val = params['subseq_val']\n        negative_advice = params['negative_advice']\n        all_sub_advices = [] # save the advices\n        all_params = [] # save the paramters\n        for lt in label_treshold:\n            for av in advice_val:\n                for sb in subseq_val:\n                    for na in negative_advice:\n                        subadvice = advice_class.return_suball_advice(sub_index, np.array(sub_preds),lt,av,sb,na)\n                        all_sub_advices.append(subadvice)\n                        all_params.append((candidate_treshold,lt,av,sb,na))\n    else: \n        # otherwise we use the normal parameters\n        subadvice = advice_class.return_suball_advice(sub_index, np.array(sub_preds))\n        preds_with_subadvice = cat_preds + subadvice\n    \n    model_names = sorted(list(model_to_path.keys()))\n    model_names.remove('Categories')\n    model_names.remove('Subcategories')\n    \n    texts = np.array(texts)\n    \n    if gridsearch:\n        # if a gridsearch was performed we store that data into the class\n        advice_class.create_gridsearch_advices(len(texts),len(all_sub_advices))\n    \n    # go through each classifier\n    for amodel_name in model_names: \n        prefix, cat_name = amodel_name.split('_') # get category name\n        cat_index = cat_names.index(cat_name)\n\n        candidates = advice_class.return_candidate(cat_index) # see how many candidates there are for this category\n        indexes = np.where(np.array(candidates))\n        if any(candidates):\n            candidate_texts = texts[candidates] # get the texts that are candidates\n            if prefix == 'cv': #cv = category-values thus we are working with the 'values' = val here\n                sub_or_val = 'val'\n            else:\n                sub_or_val = 'sub'\n                \n            #tempdict for interpretation of results\n            tdict = {}\n            \n            # get variables\n            model_ml, model_bs = params_per_model[amodel_name]\n            model_path = model_to_path[amodel_name]\n            model_size = model_size_dict[amodel_name]\n\n            model = BertForSequenceClassification.from_pretrained(model_path, num_labels=model_size) #load model\n            model.cuda()\n\n            test_dataloader = load_test_data(tokname, candidate_texts, model_bs, model_ml)\n            preds = perform_test(model, test_dataloader) # get predictions for respectiv model\n            \n            tdict['prediction'] = preds\n            tdict['index'] = indexes\n            pred_dict[amodel_name] = tdict\n\n            if gridsearch:\n                #if it is a gridsearch we go through all paramters\n                label_treshold = params['label_treshold']\n                advice_val = params['advice_val']\n                subseq_val = params['subseq_val']\n                negative_advice = params['negative_advice']\n                n = 0\n                for lt in label_treshold:\n                    for av in advice_val:\n                        for sb in subseq_val:\n                            for na in negative_advice:\n                                advice = advice_class.return_advice(cat_index, preds, lt, av, sb, na) # get advice for parametrs\n                                advice_class.save_gridsearch_advice(n,advice,sub_or_val,cat_index) # save advice in class\n                                n +=1\n\n            else:\n                # get unique parameters for this model\n                cat_params = params[cat_name]\n                label_treshold = cat_params['label_treshold'][0]\n                advice_val = cat_params['advice_val'][0]\n                subseq_val = cat_params['subseq_val'][0]\n                negative_advice = cat_params['negative_advice'][0]\n                advice = advice_class.return_advice(cat_index, preds, label_treshold,advice_val,subseq_val, negative_advice)\n                advice_class.save_advice(advice,cat_index,sub_or_val) # save advice\n                \n            # clear memory\n            K.clear_session()\n            torch.cuda.empty_cache()\n\n    \n    if gridsearch:\n        #if we performed a gridsearch we want to return all advices for all paramters\n        gridsearch_advices = advice_class.return_gridsearch_advice()\n        return cat_preds, true_labels, all_params, all_sub_advices, gridsearch_advices\n    else:\n        #otherwise we return only the predictions for each advice laer with the input parameters\n        all_combination_preditions = advice_class.return_predictions_layers()\n        return (all_combination_preditions,pred_dict)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOKNAME = 'bert-base-uncased'\ncat_preds, true_labels, all_params, all_sub,gridsearch_advices = hierarchical_classification(TOKNAME,t_my_texts,t_cats,best_params,models_paths,sub_indexes,0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_predictions(cat_preds, all_sub = None, sub = None,  val = None, all_sub_go = False, sub_go = False, val_go = False):\n    \"\"\"Function that takes base predictions and a combination of advice layer and returns the final prediction\"\"\"\n    return_predicts = cat_preds\n    if all_sub_go:\n        return_predicts = return_predicts + all_sub\n    if sub_go:\n        return_predicts = return_predicts + sub\n    if val_go:\n        return_predicts = return_predicts + val\n\n    return return_predicts\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\ndef return_combinations(cat_preds, all_sub, sub, val):\n    \"\"\"\n    Function that returns all posible combinations of advice layers\n    \"\"\"\n    all_combinations = [cat_preds]\n    for i in range(1,4):\n        combinations = itertools.combinations([1,2,3], i)\n        for combination in combinations:\n            all_sub_go = 1 in combination\n            sub_go = 2 in combination\n            val_go = 3 in combination\n            \n            all_combinations.append(return_predictions(cat_preds,all_sub,sub,val,all_sub_go,sub_go,val_go))\n    return all_combinations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gridsearch advice parameters","metadata":{}},{"cell_type":"code","source":"# Set variables and savenames\nSAVEFILE = 'treshold_gridsearch.csv' \nTOKNAME = 'bert-base-uncased'\nfinal_tresholds = [0.4,0.5,0.6,0.7,0.8,0.9]\ncandidate_treshold = [0.1,0.15,0.2,0.25,0.3,0.4]\nADVICE_NAMES = ['cat', 'sub','allsub','val','allsub_sub', 'sub_val','allsub_val','allsub_sub_val']\n#colnames are a bit different than in the paper as the anming was done differently\nCOLNAMES = ['class', 'candidate_treshold','label_treshold','advice_val','subseq_val','negative_advice','final_treshold'] + ADVICE_NAMES\nsaved = False\n\n#progress bar (rougly 570000 different paramaters)\nBAR = tf.keras.utils.Progbar(570000)\n\nif not saved:\n    result_df= pd.DataFrame(columns = COLNAMES)\nelse:\n    result_df = pd.read_csv(SAVEFILE)\n\n# we call the hierarchical_classification function for the different values of candidate_tresholds\nfor ct in candidate_treshold:\n    TOKNAME = 'bert-base-uncased'\n    cat_preds, true_labels, all_params, all_sub,gridsearch_advices = hierarchical_classification(TOKNAME,v_my_texts,v_cats,\n                                                                                             model_parameter,models_paths,sub_indexes,ct, gridsearch = True)\n    print(ct)\n    for i,param in enumerate(all_params):\n        # get parameter and respective advce\n        allsub_advice = all_sub[i]\n        sub_advice = gridsearch_advices[0][i]\n        val_advice = gridsearch_advices[1][i]\n\n        cat, allsub,sub,val,allsub_sub, allsub_val,sub_val,allsub_sub_val  = return_combinations(cat_preds,allsub_advice,sub_advice,val_advice)\n        for final_treshold in final_tresholds:\n            for cat_name in cat_names:\n                #get the prediction bools per category per advice layer\n                cat_index = cat_names.index(cat_name)\n                cat_bools = [pl>final_treshold for pl in np.array(cat)[:,cat_index]]\n                allsub_bools = [pl>final_treshold for pl in np.array(allsub)[:,cat_index]]\n                sub_bools = [pl>final_treshold for pl in np.array(sub)[:,cat_index]]\n                val_bools = [pl>final_treshold for pl in np.array(val)[:,cat_index]]\n                allsub_sub_bools = [pl>final_treshold for pl in np.array(allsub_sub)[:,cat_index]]\n                allsub_val_bools = [pl>final_treshold for pl in np.array(allsub_val)[:,cat_index]]\n                sub_val_bools = [pl>final_treshold for pl in np.array(sub_val)[:,cat_index]]\n                allsub_sub_val_bools = [pl>final_treshold for pl in np.array(allsub_sub_val)[:,cat_index]]\n                \n                #Use the true labels to calculate the respective micro f1 scores for the category for this \n                # particular set of parameters\n                true_class_labels = np.array(true_labels)[:,cat_index]\n                cat_f1_accuracy = f1_score(true_class_labels,cat_bools,average='micro')*100\n                allsub_f1_accuracy = f1_score(true_class_labels,allsub_bools,average='micro')*100\n                sub_f1_accuracy = f1_score(true_class_labels,sub_bools,average='micro')*100\n                val_f1_accuracy = f1_score(true_class_labels,val_bools,average='micro')*100\n                allsub_sub_f1_accuracy = f1_score(true_class_labels,allsub_sub_bools,average='micro')*100\n                allsub_val_f1_accuracy = f1_score(true_class_labels,allsub_val_bools,average='micro')*100\n                sub_val_f1_accuracy = f1_score(true_class_labels,sub_val_bools,average='micro')*100\n                allsub_sub_val_f1_accuracy = f1_score(true_class_labels,allsub_sub_val_bools,average='micro')*100\n                \n                # list of f1 scores per advice_layer\n                result_scores = [cat_f1_accuracy, allsub_f1_accuracy, sub_f1_accuracy,\n                                val_f1_accuracy,allsub_sub_f1_accuracy,allsub_val_f1_accuracy,\n                                sub_val_f1_accuracy,allsub_sub_val_f1_accuracy]\n                \n                # add results for cette parameter to df\n                param_with_treshold = list(param) + [final_treshold]\n                row = [cat_name] + param_with_treshold + result_scores\n                result_df.loc[len(result_df)] = row\n                BAR.add(1)\n\n# save dataframe\nresult_df.to_csv('tresholds_per_class.csv')\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using paramters\n\nThe parameter df thatw as created above has been processed and the best paramters have been selected and put into a .json file. This is done however in a different notebook, namely the data_processing notebook ('data_processing.ipynb')","metadata":{}},{"cell_type":"code","source":"import json\n# Use best parameters as saved in the json file\nwith open('../input/privbert-data/advice_parameters.json') as f:\n    best_parameters = json.load(f)\n\nexample_params = best_parameters['Data Retention']\ncandidate_treshold = example_params['candidate_treshold'][0]\nfinal_treshold = example_params['final_treshold'][0]\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testf = pd.read_csv('../input/privbert-data/op115_data/op115_test_k4.csv') # get test data for k0\nonlyfortexts = Op115OneHots(testf)\nonlyfortexts.go2(majority = True, class_tup = uniques)\n\nt_catsub,t_catval,t_subval,train_cats,t_subs,t_vals,texts = onlyfortexts.new_onehots()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perfrom test\nTOKNAME = 'bert-base-uncased'\nresults = hierarchical_classification(TOKNAME,texts,model_parameter,models_paths,sub_indexes,candidate_treshold, gridsearch = False, params=best_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save results\n\nSave results in a df and a json","metadata":{}},{"cell_type":"code","source":"true_cat_names = []\nfor cat_name in cat_names:\n    true_cat_names.append('true_'+cat_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COLNAMES = ['advice_name'] + cat_names + true_cat_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dict = {}\nprediction_df = pd.DataFrame(columns =  COLNAMES)\nfor i, amodel in enumerate(results):\n    pred_bools = [pl>final_treshold for pl in amodel]\n    val_f1_accuracy = f1_score(t_cats,pred_bools,average='micro')*100\n    val_flat_accuracy = accuracy_score(t_cats, pred_bools)*100\n    advice_name = advice_names[i]\n\n    print('F1 Validation Accuracy: ', val_f1_accuracy)\n    print('Flat Validation Accuracy: ', val_flat_accuracy)\n    \n    clf_report = classification_report(t_cats,pred_bools, target_names = cat_names, output_dict =True)\n    result_dict[advice_name] = clf_report\n   # n = 0\n    for j in range(len(amodel)):\n        preds = amodel[j]\n        trues = t_cats[j]\n        newrow = [advice_name] + list(preds) + list(trues)\n        #if n <5:\n            #print(newrow)\n           # n+=1\n    \n        prediction_df.loc[len(prediction_df)] = newrow\n\nprint(len(t_cats * 8))\nprint(len(prediction_df))\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('test_results.json', 'w') as f:\n    json.dump(result_dict, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_df.to_csv(\"predictions_k0.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}